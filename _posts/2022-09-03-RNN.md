---
layout: single
title:  "Recurrent Neural Network"
categories: coding
tag: [blog, python, coding, Machine_Learning, numpy, matplotlib, keras]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# 순환 신경망 (Recurrent Neural Network, RNN)

- **순서가 있는 데이터**를 입력으로 받음

- 변화하는 입력에 대한 출력을 얻음

- 시계열(날씨, 주가 등), 자연어와 같이 **시간의 흐름에 따라 변화하고, 그 변화가 의미를 갖는 데이터** 

## Feed Forward Network vs Recurrent Network

- Feed Forward Net (앞먹임 구조)
  - 일반적인 구조의 신경망

  - 입력 → 은닉 → 출력층 으로 이어지는 단방향 구조

  - 이전 스텝의 출력의 영향을 받지 않음

- Recurrent Net (되먹임 구조)
  - 이전 층(Layer), 또는 스텝의 출력이 다시 입력으로 연결되는 신경망 구조

  - 각 스텝마다 이전 상태를 기억 시스템(Memory System)  

  - 현재 상태가 이전 상태에 종속

  <br>

  <img src="https://www.researchgate.net/profile/Engin_Pekel/publication/315111480/figure/fig1/AS:472548166115333@1489675670530/Feed-forward-and-recurrent-ANN-architecture.png">

  <sub>출처: https://www.researchgate.net/figure/Feed-forward-and-recurrent-ANN-architecture_fig1_315111480</sub>



## 순환 신경망 구조

<img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="700">

<br>

- 입력 $x_t$에서 $t$는 시각을 뜻함

- $X_0$에 대한 출력 $Y_0$이 다음 레이어에 전달

- 각각의 입력에 대해 출력은 해당 레이어대로 출력값을 반환

## 순환 신경망의 다양한 구조

<img src="https://static.packt-cdn.com/products/9781789346640/graphics/2d4a64ef-9cf9-4b4a-9049-cb9de7a07f89.png">
  
  <sub>출처: https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789346640/11/ch11lvl1sec80/introduction</sub>

- one to one
  - RNN

- one to many
  - Image Captioning 

  - 이미지에 대한 설명 생성

- many to one
  - Sentiment Classification

  - 문장의 긍정/부정을 판단하는 감정 분석

- many to many
  - Machine Translation

  - 하나의 언어를 다른 언어로 번역하는 기계 번역

- many to many
  - Video Classification(Frame Level)




## 두 가지 정보(현재 입력, 이전 시각의 출력)을 처리하는 수식
$\qquad h_t = tanh ( \ h_{t-1} W_h \ + \ x_t W_x + b) $

- $W_x$ : 입력 $x$를 출력 $h$로 변환하기 위한 가중치

- $W_h$ : 다음 시각의 출력으로 변환하기 위한 가중치

- $h$는 '상태'를 기억

- $h_t \ $를 은닉 상태(hidden state) 또는 은닉 상태 벡터(hidden state vector)라고도 불림

  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>

## 순환 신경망 레이어 (RNN Layer)

- 입력: `(timesteps, input_features)`

- 출력: `(timesteps, output_features)`


```python
import numpy as np
```


```python
timesteps = 100
input_features = 32
output_features = 64

inputs = np.random.random((timesteps, input_features))

state_t = np.zeros((output_features, ))

W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_features, ))

sucesive_outputs = []

for input_t in inputs:
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)
    sucesive_outputs.append(output_t)
    state_t = output_t
    
final_output_sequence = np.stack(sucesive_outputs, axis=0)
```

## 케라스의 순환층
- `SimpleRNN` layer

- 입력: `(batch_size, timesteps, input_features)`

- 출력
  - `return_sequences`로 결정할 수 있음
  
  - 3D 텐서
    - 타임스텝의 출력을 모은 전체 시퀀스를 반환

    - `(batch_size, timesteps, output_features)`

  - 2D 텐서
    - 입력 시퀀스에 대한 마지막 출력만 반환

    - `(batch_size, output_features)`



```python
from tensorflow.keras.layers import SimpleRNN, Embedding
from tensorflow.keras.models import Sequential
```


```python
model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32))
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding (Embedding)       (None, None, 32)          320000    
                                                                     
     simple_rnn (SimpleRNN)      (None, 32)                2080      
                                                                     
    =================================================================
    Total params: 322,080
    Trainable params: 322,080
    Non-trainable params: 0
    _________________________________________________________________
    


```python
model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32, return_sequences=True))
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding_1 (Embedding)     (None, None, 32)          320000    
                                                                     
     simple_rnn_1 (SimpleRNN)    (None, None, 32)          2080      
                                                                     
    =================================================================
    Total params: 322,080
    Trainable params: 322,080
    Non-trainable params: 0
    _________________________________________________________________
    

- 네트워크의 표현력을 증가시키기 위해 여러 개의 순환층을 차례대로 쌓는 것이 유용할 때가 있음

  - 이런 설정에서는 중간층들이 전체 출력 시퀀스를 반환하도록 설정


```python
model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32))
model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding_2 (Embedding)     (None, None, 32)          320000    
                                                                     
     simple_rnn_2 (SimpleRNN)    (None, None, 32)          2080      
                                                                     
     simple_rnn_3 (SimpleRNN)    (None, None, 32)          2080      
                                                                     
     simple_rnn_4 (SimpleRNN)    (None, None, 32)          2080      
                                                                     
     simple_rnn_5 (SimpleRNN)    (None, 32)                2080      
                                                                     
    =================================================================
    Total params: 328,320
    Trainable params: 328,320
    Non-trainable params: 0
    _________________________________________________________________
    

## IMDB 데이터 적용

### 데이터 로드


```python
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence
```


```python
num_words=10000
max_len = 500
batch_size = 32

(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=num_words)
print(len(input_train))
print(len(input_test))

input_train = sequence.pad_sequences(input_train, maxlen=max_len)
input_test = sequence.pad_sequences(input_test, maxlen=max_len)
print(input_train.shape)
print(input_test.shape)
```

    25000
    25000
    (25000, 500)
    (25000, 500)
    

### 모델 구성


```python
from tensorflow.keras.layers import Dense
```


```python
model = Sequential()
model.add(Embedding(num_words, 32))
model.add(SimpleRNN(32))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['acc'])
model.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding_3 (Embedding)     (None, None, 32)          320000    
                                                                     
     simple_rnn_6 (SimpleRNN)    (None, 32)                2080      
                                                                     
     dense (Dense)               (None, 1)                 33        
                                                                     
    =================================================================
    Total params: 322,113
    Trainable params: 322,113
    Non-trainable params: 0
    _________________________________________________________________
    

### 모델 학습


```python
history = model.fit(input_train, y_train,
                   epochs=10,
                   batch_size=128,
                   validation_split=0.2)
```

    Epoch 1/10
    157/157 [==============================] - 21s 128ms/step - loss: 0.6626 - acc: 0.5871 - val_loss: 0.5122 - val_acc: 0.7608
    Epoch 2/10
    157/157 [==============================] - 20s 125ms/step - loss: 0.4367 - acc: 0.8073 - val_loss: 0.3916 - val_acc: 0.8328
    Epoch 3/10
    157/157 [==============================] - 19s 122ms/step - loss: 0.3154 - acc: 0.8698 - val_loss: 0.3746 - val_acc: 0.8524
    Epoch 4/10
    157/157 [==============================] - 18s 117ms/step - loss: 0.2491 - acc: 0.9022 - val_loss: 0.3628 - val_acc: 0.8484
    Epoch 5/10
    157/157 [==============================] - 18s 115ms/step - loss: 0.1984 - acc: 0.9237 - val_loss: 0.4204 - val_acc: 0.8408
    Epoch 6/10
    157/157 [==============================] - 18s 113ms/step - loss: 0.1474 - acc: 0.9456 - val_loss: 0.3871 - val_acc: 0.8666
    Epoch 7/10
    157/157 [==============================] - 18s 117ms/step - loss: 0.1189 - acc: 0.9589 - val_loss: 0.6136 - val_acc: 0.7390
    Epoch 8/10
    157/157 [==============================] - 18s 116ms/step - loss: 0.1030 - acc: 0.9648 - val_loss: 0.5127 - val_acc: 0.8356
    Epoch 9/10
    157/157 [==============================] - 15s 93ms/step - loss: 0.0820 - acc: 0.9719 - val_loss: 0.5403 - val_acc: 0.8330
    Epoch 10/10
    157/157 [==============================] - 14s 91ms/step - loss: 0.0543 - acc: 0.9817 - val_loss: 0.5175 - val_acc: 0.8444
    

### 시각화


```python
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
```


```python
loss = history.history['loss']
val_loss = history.history['val_loss']
acc = history.history['acc']
val_acc = history.history['val_acc']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'b--', label='training loss')
plt.plot(epochs, val_loss, 'r:', label='validation loss')
plt.grid()
plt.legend()

plt.figure()
plt.plot(epochs, acc, 'b--', label='training accuracy')
plt.plot(epochs, val_acc, 'r:', label='validation accuracy')
plt.grid()
plt.legend()

plt.show()
```


    
![png](/images/2022-09-03-RNN/output_25_0.png)
    



    
![png](/images/2022-09-03-RNN/output_25_1.png)
    



```python
model.evaluate(input_test, y_test)
```

    782/782 [==============================] - 15s 19ms/step - loss: 0.5346 - acc: 0.8350
    




    [0.5346211194992065, 0.8349599838256836]



- 전체 시퀀스가 아니라 순서대로 500개의 단어만 입력했기 때문에 성능이 낮게 나옴

- SimpleRNN은 긴 시퀀스를 처리하는데 적합하지 않음

- SimpleRNN은 실전에 사용하기엔 너무 단순

- SimpleRNN은 이론적으로 시간 $t$ 에서 이전의 모든 타임스텝의 정보를 유지할 수 있지만, 실제로는 긴 시간에 걸친 의존성은 학습할 수 없음

- 그래디언트 소실 문제(vanishing gradient problem)
  - 이를 방지하기 위해 LSTM, GRU 같은 레이어 등장



# LSTM(Long Short-Term Memory)
- 장단기 메모리 알고리즘

- 나중을 위해 정보를 저장함으로써 오래된 시그널이 점차 소실되는 것을 막아줌

  <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png">

  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>

## IMDB 데이터

### 데이터 로드


```python
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
```


```python
num_words=10000
max_len = 500

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)
print(len(x_train))
print(len(x_test))

pad_x_train = sequence.pad_sequences(x_train, maxlen=max_len)
pad_x_test = sequence.pad_sequences(x_test, maxlen=max_len)
print(input_train.shape)
print(input_test.shape)
```

    25000
    25000
    (25000, 500)
    (25000, 500)
    

### 모델 구성


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding
```


```python
model = Sequential()

model.add(Embedding(num_words, 2))
model.add(LSTM(32))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['acc'])
model.summary()
```

    Model: "sequential_6"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding_6 (Embedding)     (None, None, 2)           20000     
                                                                     
     lstm_2 (LSTM)               (None, 32)                4480      
                                                                     
     dense_3 (Dense)             (None, 1)                 33        
                                                                     
    =================================================================
    Total params: 24,513
    Trainable params: 24,513
    Non-trainable params: 0
    _________________________________________________________________
    

### 모델 학습


```python
history = model.fit(pad_x_train, y_train,
                   epochs=10,
                   batch_size=128,
                   validation_split=0.2)
```

    Epoch 1/10
    157/157 [==============================] - 40s 241ms/step - loss: 0.6647 - acc: 0.6150 - val_loss: 0.6172 - val_acc: 0.7740
    Epoch 2/10
    157/157 [==============================] - 35s 221ms/step - loss: 0.4665 - acc: 0.8074 - val_loss: 0.4222 - val_acc: 0.8262
    Epoch 3/10
    157/157 [==============================] - 34s 218ms/step - loss: 0.3723 - acc: 0.8533 - val_loss: 0.5238 - val_acc: 0.8134
    Epoch 4/10
    157/157 [==============================] - 31s 197ms/step - loss: 0.3068 - acc: 0.8788 - val_loss: 0.8802 - val_acc: 0.7018
    Epoch 5/10
    157/157 [==============================] - 28s 176ms/step - loss: 0.2687 - acc: 0.8988 - val_loss: 0.3168 - val_acc: 0.8634
    Epoch 6/10
    157/157 [==============================] - 28s 177ms/step - loss: 0.2388 - acc: 0.9105 - val_loss: 0.3161 - val_acc: 0.8670
    Epoch 7/10
    157/157 [==============================] - 26s 166ms/step - loss: 0.2119 - acc: 0.9238 - val_loss: 0.2771 - val_acc: 0.8890
    Epoch 8/10
    157/157 [==============================] - 26s 163ms/step - loss: 0.1934 - acc: 0.9279 - val_loss: 0.3221 - val_acc: 0.8866
    Epoch 9/10
    157/157 [==============================] - 26s 164ms/step - loss: 0.1795 - acc: 0.9360 - val_loss: 0.2869 - val_acc: 0.8800
    Epoch 10/10
    157/157 [==============================] - 26s 165ms/step - loss: 0.1653 - acc: 0.9413 - val_loss: 0.2838 - val_acc: 0.8870
    

### 시각화


```python
loss = history.history['loss']
val_loss = history.history['val_loss']
acc = history.history['acc']
val_acc = history.history['val_acc']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'b--', label='training loss')
plt.plot(epochs, val_loss, 'r:', label='validation loss')
plt.grid()
plt.legend()

plt.figure()
plt.plot(epochs, acc, 'b--', label='training accuracy')
plt.plot(epochs, val_acc, 'r:', label='validation accuracy')
plt.grid()
plt.legend()

plt.show()
```


    
![png](/images/2022-09-03-RNN/output_39_0.png)
    



    
![png](/images/2022-09-03-RNN/output_39_1.png)
    


### 모델 평가


```python
model.evaluate(pad_x_test, y_test)
```

    782/782 [==============================] - 22s 28ms/step - loss: 0.3009 - acc: 0.8787
    




    [0.30093255639076233, 0.8787199854850769]



# GRU (Gated Recurrent Unit)
- LSTM을 더 단순하게 만든 구조

- 기억 셀은 없고, 시간방향으로 전파하는 것은 은닉 상태만 있음

- reset gate
  - 과거의 은닉 상태를 얼마나 무시할지 결정

  - $r$ 값이 결정

- update gate
  -  은닉 상태를 갱신하는 게이트  

  - LSTM의 forget, input gate 역할을 동시에 함
  
  <img src="https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png" width="500">

<sub>출처: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub>

  ### $\qquad z = \sigma (x_t W^{(z)}_x + h_{t-1} W^{(z)}_h + b^{(z)} \\ 
  \qquad r = \sigma (x_t W^{(r)}_x + h_{t-1} W^{(r)}_h + b^{(r)}) \\
  \qquad \tilde{i} = tanh (x_t W^{(i)}_x + (r \odot h_{t-1}) W^{(i)}_h + b ) \\
  \qquad h_t = (1 - z) \odot h_{t-1} + z \odot \tilde{h}$


## Reuters 데이터

- IMDB와 유사한 데이터셋(텍스트 데이터)

- 46개의 상호 배타적인 토픽으로 이루어진 데이터셋 
  - 다중 분류 문제


### 데이터셋 로드


```python
from tensorflow.keras.datasets import reuters
```


```python
num_words=10000
(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words)

print(x_train.shape)
print(y_train.shape)

print(x_test.shape)
print(y_test.shape)
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz
    2113536/2110848 [==============================] - 0s 0us/step
    2121728/2110848 [==============================] - 0s 0us/step
    (8982,)
    (8982,)
    (2246,)
    (2246,)
    

### 데이터 전처리 및 확인


```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
```


```python
max_len = 300
```


```python
pad_x_train = pad_sequences(x_train, maxlen=max_len)
pad_x_test = pad_sequences(x_test, maxlen=max_len)
```


```python
pad_x_train[0]
```




    array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
              0,    0,    0,    0,    1,    2,    2,    8,   43,   10,  447,
              5,   25,  207,  270,    5, 3095,  111,   16,  369,  186,   90,
             67,    7,   89,    5,   19,  102,    6,   19,  124,   15,   90,
             67,   84,   22,  482,   26,    7,   48,    4,   49,    8,  864,
             39,  209,  154,    6,  151,    6,   83,   11,   15,   22,  155,
             11,   15,    7,   48,    9, 4579, 1005,  504,    6,  258,    6,
            272,   11,   15,   22,  134,   44,   11,   15,   16,    8,  197,
           1245,   90,   67,   52,   29,  209,   30,   32,  132,    6,  109,
             15,   17,   12])



### 모델 구성
- LSTM 레이어도 SimpleRNN과 같이 `return_sequences` 인자 사용가능


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Embedding
```


```python
model = Sequential()
model.add(Embedding(input_dim=num_words, output_dim=256))
model.add(GRU(256, return_sequences=True))
model.add(GRU(128))
model.add(Dense(46, activation='softmax'))

model.compile(optimizer='adam',
             loss='sparse_categorical_crossentropy',
             metrics=['acc'])
model.summary()
```

    Model: "sequential_8"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     embedding_8 (Embedding)     (None, None, 256)         2560000   
                                                                     
     gru_1 (GRU)                 (None, None, 256)         394752    
                                                                     
     gru_2 (GRU)                 (None, 128)               148224    
                                                                     
     dense_5 (Dense)             (None, 46)                5934      
                                                                     
    =================================================================
    Total params: 3,108,910
    Trainable params: 3,108,910
    Non-trainable params: 0
    _________________________________________________________________
    

### 모델 학습


```python
history = model.fit(pad_x_train, y_train,
                   batch_size=32, epochs=20,
                   validation_split=0.2)
```

    Epoch 1/20
    225/225 [==============================] - 222s 964ms/step - loss: 2.0234 - acc: 0.4782 - val_loss: 1.6815 - val_acc: 0.5748
    Epoch 2/20
    225/225 [==============================] - 206s 916ms/step - loss: 1.6266 - acc: 0.5862 - val_loss: 1.6234 - val_acc: 0.5954
    Epoch 3/20
    225/225 [==============================] - 208s 926ms/step - loss: 1.3596 - acc: 0.6540 - val_loss: 1.4211 - val_acc: 0.6483
    Epoch 4/20
    225/225 [==============================] - 205s 913ms/step - loss: 1.0630 - acc: 0.7257 - val_loss: 1.3237 - val_acc: 0.6784
    Epoch 5/20
    225/225 [==============================] - 184s 816ms/step - loss: 0.8352 - acc: 0.7852 - val_loss: 1.3886 - val_acc: 0.6761
    Epoch 6/20
    225/225 [==============================] - 179s 796ms/step - loss: 0.6482 - acc: 0.8335 - val_loss: 1.4250 - val_acc: 0.6817
    Epoch 7/20
    225/225 [==============================] - 178s 791ms/step - loss: 0.5096 - acc: 0.8697 - val_loss: 1.4608 - val_acc: 0.6789
    Epoch 8/20
    225/225 [==============================] - 175s 780ms/step - loss: 0.3971 - acc: 0.9006 - val_loss: 1.4766 - val_acc: 0.6906
    Epoch 9/20
    225/225 [==============================] - 177s 787ms/step - loss: 0.3066 - acc: 0.9214 - val_loss: 1.6213 - val_acc: 0.6778
    Epoch 10/20
    225/225 [==============================] - 168s 746ms/step - loss: 0.2448 - acc: 0.9376 - val_loss: 1.6182 - val_acc: 0.6800
    Epoch 11/20
    225/225 [==============================] - 162s 721ms/step - loss: 0.2043 - acc: 0.9459 - val_loss: 1.6732 - val_acc: 0.6884
    Epoch 12/20
    225/225 [==============================] - 170s 755ms/step - loss: 0.1686 - acc: 0.9532 - val_loss: 1.7278 - val_acc: 0.6850
    Epoch 13/20
    225/225 [==============================] - 185s 821ms/step - loss: 0.1502 - acc: 0.9549 - val_loss: 1.8095 - val_acc: 0.6728
    Epoch 14/20
    225/225 [==============================] - 178s 792ms/step - loss: 0.1389 - acc: 0.9559 - val_loss: 1.8098 - val_acc: 0.6884
    Epoch 15/20
    225/225 [==============================] - 179s 795ms/step - loss: 0.1344 - acc: 0.9549 - val_loss: 1.7803 - val_acc: 0.6978
    Epoch 16/20
    225/225 [==============================] - 188s 837ms/step - loss: 0.1162 - acc: 0.9594 - val_loss: 1.9696 - val_acc: 0.6694
    Epoch 17/20
    225/225 [==============================] - 187s 831ms/step - loss: 0.1138 - acc: 0.9585 - val_loss: 1.9114 - val_acc: 0.6700
    Epoch 18/20
    225/225 [==============================] - 178s 792ms/step - loss: 0.1066 - acc: 0.9594 - val_loss: 1.9014 - val_acc: 0.6772
    Epoch 19/20
    225/225 [==============================] - 176s 784ms/step - loss: 0.0990 - acc: 0.9608 - val_loss: 1.9917 - val_acc: 0.6784
    Epoch 20/20
    225/225 [==============================] - 162s 722ms/step - loss: 0.0924 - acc: 0.9613 - val_loss: 1.9205 - val_acc: 0.6828
    

### 시각화


```python
loss = history.history['loss']
val_loss = history.history['val_loss']
acc = history.history['acc']
val_acc = history.history['val_acc']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'b--', label='training loss')
plt.plot(epochs, val_loss, 'r:', label='validation loss')
plt.grid()
plt.legend()

plt.figure()
plt.plot(epochs, acc, 'b--', label='training accuracy')
plt.plot(epochs, val_acc, 'r:', label='validation accuracy')
plt.grid()
plt.legend()

plt.show()
```


    
![png](/images/2022-09-03-RNN/output_58_0.png)
    



    
![png](/images/2022-09-03-RNN/output_58_1.png)
    


### 모델 평가



```python
model.evaluate(pad_x_test, y_test)
```

    71/71 [==============================] - 13s 176ms/step - loss: 2.0988 - acc: 0.6768
    




    [2.0987696647644043, 0.6767587065696716]


